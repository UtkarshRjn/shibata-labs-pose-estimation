{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:15:02.926899: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-27 08:15:02.976491: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-27 08:15:02.977197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-27 08:15:03.878037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MoveNet model from TensorFlow Hub\n",
    "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
    "movenet = movenet_model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for keypoints\n",
    "label = [\"nose\", \"left eye\", \"right eye\", \"left ear\", \"right ear\",\n",
    "         \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\",\n",
    "         \"left wrist\", \"right wrist\", \"left hip\", \"right hip\",\n",
    "         \"left knee\", \"right knee\", \"left ankle\", \"right ankle\"]\n",
    "\n",
    "pose_label = [\"standing\", \"sitting\", \"walking\"]\n",
    "\n",
    "score_threshold = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for DQN\n",
    "alpha = 0.001  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 32\n",
    "memory_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Neural network for Q-value approximation\n",
    "def build_model(input_shape, output_shape):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(64, input_shape=(input_shape,), activation='relu'),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(output_shape, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=alpha), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "        return [self.buffer[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform pose estimation on a single frame\n",
    "def estimate_pose(frame, movenet, label):\n",
    "\n",
    "    original_height , original_width, _ = frame.shape\n",
    "\n",
    "    input_image = tf.expand_dims(frame, axis=0)\n",
    "\n",
    "    input_image = tf.cast(tf.image.resize_with_pad(input_image, 256, 256), dtype=tf.int32)\n",
    "    _, resized_height , resized_width, _ = input_image.shape\n",
    "\n",
    "    scale_factor_height = original_height / resized_height\n",
    "    scale_factor_width = original_width / resized_width\n",
    "\n",
    "    outputs = movenet(input_image)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "\n",
    "    num_keypoints = keypoints.shape[1]  # Get the number of detected keypoints\n",
    "\n",
    "    max_key , key_val = keypoints[0,:,55].argmax(), keypoints[0,:,55].max()\n",
    "\n",
    "    max_points = keypoints[0,max_key,:]\n",
    "    max_points = max_points*256\n",
    "    max_points = max_points.astype(float)\n",
    "\n",
    "\n",
    "    keypoints_dict = {}\n",
    "    for i in range(0,len(max_points)-5,3):\n",
    "        if(max_points[i+2] > score_threshold):\n",
    "            max_points[i] = max_points[i] * scale_factor_height\n",
    "            max_points[i+1] = max_points[i+1] * scale_factor_width\n",
    "            keypoints_dict[label[i//3]] = [max_points[i+1].astype(int),max_points[i].astype(int),max_points[i+2]]\n",
    "\n",
    "    return keypoints_dict, keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to draw the predicted keypoints and connections on the frame\n",
    "def draw_pose(frame, keypoints_dict, predicted_label):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    for key in keypoints_dict:\n",
    "        x, y, score = keypoints_dict[key]\n",
    "        if score > score_threshold:  # Only plot keypoints with score above 0.2\n",
    "            cv2.circle(frame, (int(x), int(y)), 5, (0, 255, 255), -1)\n",
    "            cv2.putText(frame, key, (int(x) + 5, int(y) + 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Define connections between keypoints\n",
    "    connections = [\n",
    "        ('nose', 'left eye'), ('left eye', 'left ear'), ('nose', 'right eye'), ('right eye', 'right ear'),\n",
    "        ('nose', 'left shoulder'), ('left shoulder', 'left elbow'), ('left elbow', 'left wrist'),\n",
    "        ('nose', 'right shoulder'), ('right shoulder', 'right elbow'), ('right elbow', 'right wrist'),\n",
    "        ('left shoulder', 'left hip'), ('right shoulder', 'right hip'), ('left hip', 'right hip'),\n",
    "        ('left hip', 'left knee'), ('right hip', 'right knee'), ('left knee', 'left ankle'), ('right knee', 'right ankle')\n",
    "    ]\n",
    "\n",
    "    for start_key, end_key in connections:\n",
    "        if start_key in keypoints_dict and end_key in keypoints_dict:\n",
    "            start_point = keypoints_dict[start_key][:2]\n",
    "            end_point = keypoints_dict[end_key][:2]\n",
    "            cv2.line(frame, tuple(start_point), tuple(end_point), (0, 255, 255), 2)\n",
    "\n",
    "    # Add the predicted label text to the frame\n",
    "    cv2.putText(frame, f\"Predicted Pose: {predicted_label}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capture frames from the camera and perform pose estimation\n",
    "def pose_estimation_camera(movenet, label, model, target_model, buffer, gamma, epsilon, epsilon_min, epsilon_decay, batch_size):    \n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Perform pose estimation\n",
    "        keypoints_dict, keypoints = estimate_pose(frame, movenet, label)\n",
    "\n",
    "\n",
    "        state = keypoints[0].flatten().astype(int).tobytes()\n",
    "        state = np.frombuffer(state, dtype=int).astype(float)\n",
    "        \n",
    "              # Randomly choose an action (label) based on epsilon-greedy policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action_idx = random.choice(range(len(pose_label)))\n",
    "        else:\n",
    "            q_values = model.predict(state[np.newaxis])\n",
    "            action_idx = np.argmax(q_values[0])\n",
    "\n",
    "        predicted_idx = action_idx\n",
    "        print(action_idx)\n",
    "\n",
    "        # Get the correct label from the user\n",
    "        correct_idx = input(\"Enter the correct label: \")\n",
    "\n",
    "        # Calculate the reward\n",
    "        reward = 1 if predicted_idx == correct_idx else -1\n",
    "\n",
    "        # Perform pose estimation on the next frame\n",
    "        keypoints_next_dict, keypoints_next  = estimate_pose(frame, movenet, label)\n",
    "        next_state = keypoints_next[0].flatten().astype(int).tobytes()\n",
    "        next_state = np.frombuffer(next_state, dtype=int).astype(float)\n",
    "\n",
    "        # Add experience to replay buffer\n",
    "        buffer.add((state, action_idx, reward, next_state))\n",
    "\n",
    "        # Update Q-values using mini-batch from replay buffer\n",
    "        if len(buffer.buffer) >= batch_size:\n",
    "            minibatch = buffer.sample(batch_size)\n",
    "            for s, a, r, s_next in minibatch:\n",
    "                target = r\n",
    "                if not (s_next == 0).all():\n",
    "                    target = r + gamma * np.amax(target_model.predict(s_next[np.newaxis])[0])\n",
    "                target_f = model.predict(s[np.newaxis])\n",
    "                target_f[0][a] = target\n",
    "                model.fit(s[np.newaxis], target_f, epochs=1, verbose=0)\n",
    "\n",
    "            # Update the target model\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "        # Decay epsilon\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        # Draw keypoints and connections on the frame\n",
    "        frame_with_pose = draw_pose(frame, keypoints_dict, pose_label[predicted_idx])\n",
    "\n",
    "        # Display the frame with pose estimation\n",
    "        cv2.imshow('Pose Estimation', frame_with_pose)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# Initialize DQN\n",
    "input_shape = 51  # Assuming 17 keypoints with (x, y, score) each\n",
    "output_shape = len(pose_label)\n",
    "model = build_model(input_shape, output_shape)\n",
    "target_model = build_model(input_shape, output_shape)\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize replay buffer\n",
    "buffer = ReplayBuffer(memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/utkarsh-ranjan/Documents/shibata-labs/shibata-lab-pose-estimation/movenet/py3.8/lib/python3.8/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Run the camera pose estimation function with DQN\n",
    "pose_estimation_camera(movenet, label, model, target_model, buffer, gamma, epsilon, epsilon_min, epsilon_decay, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
