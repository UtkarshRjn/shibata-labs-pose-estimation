{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 12:27:25.149960: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-26 12:27:25.198730: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-26 12:27:25.199592: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-26 12:27:26.205932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MoveNet model from TensorFlow Hub\n",
    "model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for keypoints\n",
    "label = [\"nose\", \"left eye\", \"right eye\", \"left ear\", \"right ear\",\n",
    "         \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\",\n",
    "         \"left wrist\", \"right wrist\", \"left hip\", \"right hip\",\n",
    "         \"left knee\", \"right knee\", \"left ankle\", \"right ankle\"]\n",
    "\n",
    "score_threshold = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform pose estimation on a single frame\n",
    "def estimate_pose(frame, movenet, label):\n",
    "\n",
    "    original_height , original_width, _ = frame.shape\n",
    "\n",
    "    input_image = tf.expand_dims(frame, axis=0)\n",
    "\n",
    "    input_image = tf.cast(tf.image.resize_with_pad(input_image, 256, 256), dtype=tf.int32)\n",
    "    _, resized_height , resized_width, _ = input_image.shape\n",
    "\n",
    "    scale_factor_height = original_height / resized_height\n",
    "    scale_factor_width = original_width / resized_width\n",
    "\n",
    "    outputs = movenet(input_image)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "\n",
    "    num_keypoints = keypoints.shape[1]  # Get the number of detected keypoints\n",
    "\n",
    "    max_key , key_val = keypoints[0,:,55].argmax(), keypoints[0,:,55].max()\n",
    "\n",
    "    max_points = keypoints[0,max_key,:]\n",
    "    max_points = max_points*256\n",
    "    max_points = max_points.astype(float)\n",
    "\n",
    "\n",
    "    keypoints_dict = {}\n",
    "    for i in range(0,len(max_points)-5,3):\n",
    "        if(max_points[i+2] > score_threshold):\n",
    "            max_points[i] = max_points[i] * scale_factor_height\n",
    "            max_points[i+1] = max_points[i+1] * scale_factor_width\n",
    "            keypoints_dict[label[i//3]] = [max_points[i+1].astype(int),max_points[i].astype(int),max_points[i+2]]\n",
    "\n",
    "    return keypoints_dict, keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to draw the predicted keypoints and connections on the frame\n",
    "def draw_pose(frame, keypoints_dict, label):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    for key in keypoints_dict:\n",
    "        x, y, score = keypoints_dict[key]\n",
    "        if score > score_threshold:  # Only plot keypoints with score above 0.2\n",
    "            cv2.circle(frame, (int(x), int(y)), 5, (0, 255, 255), -1)\n",
    "            cv2.putText(frame, key, (int(x) + 5, int(y) + 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Define connections between keypoints\n",
    "    connections = [\n",
    "        ('nose', 'left eye'), ('left eye', 'left ear'), ('nose', 'right eye'), ('right eye', 'right ear'),\n",
    "        ('nose', 'left shoulder'), ('left shoulder', 'left elbow'), ('left elbow', 'left wrist'),\n",
    "        ('nose', 'right shoulder'), ('right shoulder', 'right elbow'), ('right elbow', 'right wrist'),\n",
    "        ('left shoulder', 'left hip'), ('right shoulder', 'right hip'), ('left hip', 'right hip'),\n",
    "        ('left hip', 'left knee'), ('right hip', 'right knee'), ('left knee', 'left ankle'), ('right knee', 'right ankle')\n",
    "    ]\n",
    "\n",
    "    for start_key, end_key in connections:\n",
    "        if start_key in keypoints_dict and end_key in keypoints_dict:\n",
    "            start_point = keypoints_dict[start_key][:2]\n",
    "            end_point = keypoints_dict[end_key][:2]\n",
    "            cv2.line(frame, tuple(start_point), tuple(end_point), (0, 255, 255), 2)\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capture frames from the camera and perform pose estimation\n",
    "def pose_estimation_camera(movenet, label, q_table, alpha, gamma, epsilon):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Perform pose estimation\n",
    "        keypoints_dict, keypoints = estimate_pose(frame, movenet, label)\n",
    "\n",
    "\n",
    "        state = keypoints[0].flatten().astype(int).tobytes()\n",
    "        \n",
    "        # Initialize Q-values for the new state if not already initialized\n",
    "        if state not in q_table:\n",
    "            q_table[state] = {lbl: 0 for lbl in label}\n",
    "\n",
    "        # Randomly choose an action (label) based on epsilon-greedy policy\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            predicted_label = random.choice(label)\n",
    "        else:\n",
    "            state = keypoints[0].flatten().astype(int).tobytes()\n",
    "            predicted_label = max(q_table[state], key=q_table[state].get)\n",
    "\n",
    "        # Get the correct label from the user\n",
    "        correct_label = input(\"Enter the correct label: \")\n",
    "\n",
    "        # Calculate the reward\n",
    "        reward = 1 if predicted_label == correct_label else -1\n",
    "\n",
    "        # Update Q-table\n",
    "        state = keypoints[0].flatten().astype(int).tobytes()\n",
    "        if correct_label not in q_table[state]:\n",
    "            q_table[state][correct_label] = 0\n",
    "        q_table[state][predicted_label] = q_table[state][predicted_label] + alpha * (\n",
    "            reward + gamma * max(q_table[state].values()) - q_table[state][predicted_label]\n",
    "        )\n",
    "\n",
    "        # Draw keypoints and connections on the frame\n",
    "        frame_with_pose = draw_pose(frame, keypoints_dict, label)\n",
    "\n",
    "        # Display the frame with pose estimation\n",
    "        cv2.imshow('Pose Estimation', frame_with_pose)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Initialize Q-table\n",
    "q_table = defaultdict(lambda: defaultdict(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pose_estimation_camera' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the camera pose estimation function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpose_estimation_camera\u001b[49m(movenet, label, q_table, alpha, gamma, epsilon)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pose_estimation_camera' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the camera pose estimation function\n",
    "pose_estimation_camera(movenet, label, q_table, alpha, gamma, epsilon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
